{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pybedtools import BedTool\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/private8/Projects/itamar/ZNF/Sra_GSE73211_35_samples/REDI/nodups_cov5_HE_PLUS_STAR100len/Edits_only/\"\n",
    "all_files = []\n",
    "for outf in [path+f for f in os.listdir(path) if f.endswith('.out.tab')]:\n",
    "    with open(outf) as ffd:\n",
    "        DictReader_obj = csv.DictReader(ffd, delimiter=\"\\t\")\n",
    "        # content = ffd.readlines()\n",
    "        file_as_list_dict = [line for line in DictReader_obj]\n",
    "        all_files.append(file_as_list_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count presence of every site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_counter_dict = {}\n",
    "for sample in all_files:\n",
    "    for site in sample:\n",
    "        siteID = site[\"Region\"]+\"_\"+site[\"Position\"]\n",
    "        if siteID not in sites_counter_dict.keys():\n",
    "            sites_counter_dict[siteID] = 1\n",
    "        else:\n",
    "            sites_counter_dict[siteID] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create list of recurrent sites and list of those sites for every sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurent_sites_pos = [str(key) for key in sites_counter_dict.keys() if sites_counter_dict[key] > 10]\n",
    "all_files_recurent_sites = []\n",
    "for sample in all_files:\n",
    "    samples_recurrent_sites = [site for site in sample if (site[\"Region\"]+\"_\"+site[\"Position\"]) in recurent_sites_pos]\n",
    "    all_files_recurent_sites.append(samples_recurrent_sites.copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter snps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter 100% editing sites\n",
    "all_files_recurent_sites_no_snp =[]\n",
    "for sample in all_files_recurent_sites:\n",
    "        temp = [site for site in sample if 0.001 < float(site[\"Frequency\"]) < 0.98] \n",
    "        # TODO also filte 45 -55%?\n",
    "        #   = [site for site in sample if ((0.001 < float(site[\"Frequency\"]) < 0.98) and  (0.55 < float(site[\"Frequency\"]) or float(site[\"Frequency\"]) < 0.45))]\n",
    "        all_files_recurent_sites_no_snp.append(temp.copy())    \n",
    "counter_sites = sum([len(sample_sites) for sample_sites in all_files_recurent_sites_no_snp])\n",
    "#fiter snps from dbsnp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter snps from dbsnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  23358 \n",
      " uniq:  1075\n"
     ]
    }
   ],
   "source": [
    "#create bed file from results\n",
    "temp_F_path = path+\"temp_pos.bed\"\n",
    "sites_as_bed =  [site[\"Region\"]+\"\\t\"+str(int(site[\"Position\"])-1)+\"\\t\"+site[\"Position\"]+\"\\n\" for sample in all_files_recurent_sites for site in sample]\n",
    "uniq = set(sites_as_bed)\n",
    "with open(temp_F_path, \"w\") as outFD:\n",
    "    outFD.writelines(uniq)\n",
    "# intersect with snps\n",
    "snps = BedTool(\"/private/common/Data/dbSNP-153/hg38-dbSNP153-common.bed\")\n",
    "sites = BedTool(temp_F_path)\n",
    "result_path = path+\"temp_result.bed\"\n",
    "# write and read results\n",
    "(sites-snps).saveas(result_path)\n",
    "with open(result_path, \"r\") as resFD:\n",
    "    as_bed_no_snp = resFD.readlines()\n",
    "# remove temp file\n",
    "os.remove(result_path)\n",
    "os.remove(temp_F_path)\n",
    "# save the result as positions\n",
    "no_snp_positions = [(pos.split(sep=\"\\t\")[0], pos.split(sep=\"\\t\")[2][:-1])for pos in as_bed_no_snp]\n",
    "all_files_recurent_sites_sits_no_snp = []\n",
    "# save for every sample the passed sites\n",
    "for sample in all_files_recurent_sites:\n",
    "    all_files_recurent_sites_sits_no_snp.append([site for site in sample if (site[\"Region\"], site[\"Position\"]) in no_snp_positions])\n",
    "# save list of all reults (contains duplicats!)\n",
    "all_sites_list_no_snp = [site for sample in all_files_recurent_sites_sits_no_snp for site in sample]\n",
    "# create uniq list of all sites\n",
    "uniq_sites = []\n",
    "sites_dict = dict()\n",
    "already = set()\n",
    "for site in all_sites_list_no_snp:\n",
    "    # make set from subs\n",
    "    site[\"AllSubs\"] = set(site[\"AllSubs\"].split(\" \"))\n",
    "    # keep only A2G sites\n",
    "    if \"AG\" not in site[\"AllSubs\"]:\n",
    "        continue\n",
    "\n",
    "\n",
    "    site[\"BaseCount[A,C,G,T]\"]  = np.array([int(x) for x in site[\"BaseCount[A,C,G,T]\"].replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")])\n",
    "    site['Coverage-q25'] = float(site['Coverage-q25'])\n",
    "    site['MeanQ'] = float(site['MeanQ'])\n",
    "\n",
    "    siteID = site[\"Region\"]+\"_\"+site[\"Position\"] \n",
    "    if siteID not in sites_dict:\n",
    "        sites_dict[siteID] = site\n",
    "    else:\n",
    "        sites_dict[siteID][\"AllSubs\"].union(site[\"AllSubs\"])\n",
    "        sites_dict[siteID][\"BaseCount[A,C,G,T]\"] += site[\"BaseCount[A,C,G,T]\"]\n",
    "        total_cov = sites_dict[siteID]['Coverage-q25'] + site['Coverage-q25']\n",
    "        sites_dict[siteID]['MeanQ'] = round(site['MeanQ']*(site['Coverage-q25']/total_cov) + sites_dict[siteID]['MeanQ']*(sites_dict[siteID]['Coverage-q25']/total_cov),1)\n",
    "        sites_dict[siteID]['Coverage-q25'] += site['Coverage-q25']\n",
    "    # if (site[\"Region\"], site[\"Position\"]) not in already:\n",
    "    #     already.add((site[\"Region\"], site[\"Position\"]))\n",
    "    #     uniq_sites.append(site)\n",
    "# update frequency\n",
    "for site in sites_dict.values():\n",
    "    site[\"Frequency\"] = round(site[\"BaseCount[A,C,G,T]\"][2]/np.sum(site[\"BaseCount[A,C,G,T]\"]),4)\n",
    "# convert dict to list\n",
    "uniq_sites = [site for site in sites_dict.values()]\n",
    "print(\"total: \",len(all_sites_list_no_snp),\"\\n uniq: \", len(sites_dict))\n",
    "#save file\n",
    "with open(path+\"all_uniq_recurent_sites.known.tab\", \"w\") as outFd:\n",
    "    writer = csv.DictWriter(outFd, delimiter=\"\\t\",fieldnames=list(uniq_sites[0].keys()))\n",
    "    writer.writeheader()\n",
    "    writer.writerows(uniq_sites) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.606565656, 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3566c55396aa67d6d450caee2c72072e0a80bcedc801748038b289e6d3e79506"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lamootHW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
