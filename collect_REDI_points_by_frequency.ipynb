{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "# raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/private8/Projects/itamar/ZNF/Sra_GSE73211_35_samples/REDI/curated/nodups_cov5_HE_PLUS_STAR100len/Edits_only/\"\n",
    "all_samples = []\n",
    "for outf in [path+f for f in os.listdir(path) if f.endswith('.out.tab')]:\n",
    "    with open(outf,'r') as ffd:\n",
    "        DictReader_obj = csv.DictReader(ffd, delimiter=\"\\t\")\n",
    "        # content = ffd.readlines()\n",
    "        file_as_list_dict = [line for line in DictReader_obj]\n",
    "        all_samples.append(file_as_list_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#filter 100% editing sites\n",
    "all_files_sites_no_snp_raw =[]\n",
    "for sample in all_samples:\n",
    "        # temp = [site for site in sample if 0.001 < float(site[\"Frequency\"]) < 0.98] \n",
    "        # TODO also filte 45 -55%?\n",
    "        temp  = [site for site in sample if ((0.001 < float(site[\"Frequency\"]) < 0.98) and  (0.55 < float(site[\"Frequency\"]) or float(site[\"Frequency\"]) < 0.45))]\n",
    "        all_files_sites_no_snp_raw.append(temp.copy())\n",
    "\n",
    "#fiter snps from dbsnp\n",
    "temp_F_path = path+\"temp_pos.bed\"\n",
    "sites_as_bed =  [site[\"Region\"]+\"\\t\"+str(int(site[\"Position\"])-1)+\"\\t\"+site[\"Position\"]+\"\\n\" for sample in all_files_sites_no_snp_raw for site in sample]\n",
    "uniq = set(sites_as_bed)\n",
    "#print(len(uniq))\n",
    "with open(temp_F_path, \"w\") as outFD:\n",
    "    outFD.writelines(uniq)\n",
    "# intersect with snps\n",
    "snps = BedTool(\"/private/common/Data/dbSNP-153/hg38-dbSNP153-common.bed\")\n",
    "sites = BedTool(temp_F_path)\n",
    "result_path = path+\"temp_result.bed\"\n",
    "# write and read results\n",
    "(sites-snps).saveas(result_path)\n",
    "with open(result_path, \"r\") as resFD:\n",
    "    as_bed_no_snp = resFD.readlines()\n",
    "#print(len(as_bed_no_snp))\n",
    "# remove temp file\n",
    "os.remove(result_path)\n",
    "os.remove(temp_F_path)\n",
    "# save the result as positions\n",
    "no_snp_positions = [(pos.split(sep=\"\\t\")[0], pos.split(sep=\"\\t\")[2][:-1])for pos in as_bed_no_snp]\n",
    "all_files_sites_no_snp = []\n",
    "# save for every sample the passed sites\n",
    "for sample in all_files_sites_no_snp_raw:\n",
    "    all_files_sites_no_snp.append([site for site in sample if (site[\"Region\"], site[\"Position\"]) in no_snp_positions])\n",
    "# save list of all reults (contains duplicats!)\n",
    "all_sites_list_no_snp = [site for sample in all_files_sites_no_snp for site in sample]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create table of all sites composing data for every uniq site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  137393 \n",
      " uniq:  65300\n"
     ]
    }
   ],
   "source": [
    "# create uniq list of all sites\n",
    "uniq_sites = []\n",
    "sites_dict = dict()\n",
    "already = set()\n",
    "for site_org in all_sites_list_no_snp:\n",
    "    site=site_org.copy()\n",
    "    # make set from subs\n",
    "    site[\"AllSubs\"] = set(site[\"AllSubs\"].split(\" \"))\n",
    "    site[\"BaseCount[A,C,G,T]\"]  = np.array([int(x) for x in site[\"BaseCount[A,C,G,T]\"].replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")])\n",
    "    site['Coverage-q25'] = float(site['Coverage-q25'])\n",
    "    site['MeanQ'] = float(site['MeanQ'])\n",
    "    site['Frequency'] = float(site['Frequency'])\n",
    "    siteID = site[\"Region\"]+\"_\"+site[\"Position\"]\n",
    "    if siteID not in sites_dict:\n",
    "        sites_dict[siteID] = site.copy()\n",
    "    else:\n",
    "        sites_dict[siteID][\"AllSubs\"].update(site[\"AllSubs\"].copy())\n",
    "        sites_dict[siteID][\"BaseCount[A,C,G,T]\"] += site[\"BaseCount[A,C,G,T]\"]\n",
    "        total_cov = sites_dict[siteID]['Coverage-q25'] + site['Coverage-q25']\n",
    "        sites_dict[siteID]['MeanQ'] = round(site['MeanQ']*(site['Coverage-q25']/total_cov) + sites_dict[siteID]['MeanQ']*(sites_dict[siteID]['Coverage-q25']/total_cov),1)\n",
    "        sites_dict[siteID]['Frequency']= round(site['Frequency']*(site['Coverage-q25']/total_cov) + sites_dict[siteID]['Frequency']*(sites_dict[siteID]['Coverage-q25']/total_cov),4)\n",
    "        sites_dict[siteID]['Coverage-q25'] += site['Coverage-q25']\n",
    "    \n",
    "# after collecting all sites ->calculations\n",
    "for site in sites_dict.values():\n",
    "    # calc frequncy again\n",
    "    b_c=site[\"BaseCount[A,C,G,T]\"].copy()\n",
    "    n_options = ['A','C','G','T']\n",
    "    #find the original nucleotide without editing\n",
    "    orig_pos=np.argmax(b_c)\n",
    "    orig_nuc=n_options[orig_pos]\n",
    "    b_c[orig_pos]=0\n",
    "    # find the edited to\n",
    "    new_pos=np.argmax(b_c)\n",
    "    new_nuc=n_options[new_pos]\n",
    "    # save mismatch type and Frequency\n",
    "    site['MM']=orig_nuc+new_nuc\n",
    "    site['CountFrequency']=round(site[\"BaseCount[A,C,G,T]\"][new_pos]/np.sum(site[\"BaseCount[A,C,G,T]\"]),4)\n",
    "    # caculate AG frequncy by base count\n",
    "    site['CountAGFrequency']=round(site[\"BaseCount[A,C,G,T]\"][2]/np.sum(site[\"BaseCount[A,C,G,T]\"]),4)\n",
    "    # convert BaseCount to string\n",
    "    site[\"BaseCount[A,C,G,T]\"] = \";\".join([str(x) for x in site[\"BaseCount[A,C,G,T]\"].tolist()])\n",
    "    site[\"AllSubs\"] = \";\".join(site[\"AllSubs\"])\n",
    "# convert dict to list\n",
    "uniq_sites = list(sites_dict.values())\n",
    "print(\"total: \",len(all_sites_list_no_snp),\"\\n uniq: \", len(uniq_sites))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for site in all_sites_list_no_snp:\n",
    "#     if site['Position'] == \"54375862\":\n",
    "#         print(site)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  137393 \n",
      " uniq:  9769\n"
     ]
    }
   ],
   "source": [
    "# filter by cov > 100 and some MM's freq > 0.01\n",
    "uniq_sites_filtered = [site for site in uniq_sites if (site[\"CountFrequency\"] >= 0.01 and site['Coverage-q25'] > 100)]\n",
    "AG_uniq_sites_filtered = [site for site in uniq_sites if (site[\"CountAGFrequency\"] >= 0.01 and site['Coverage-q25'] > 100)] \n",
    "print(\"total: \",len(all_sites_list_no_snp),\"\\n uniq: \", len(uniq_sites_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG    5551\n",
      "AT    2339\n",
      "AC    1879\n",
      "Name: MM, dtype: int64\n",
      "AG    0.568226\n",
      "AT    0.239431\n",
      "AC    0.192343\n",
      "Name: MM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# count number of each MM\n",
    "df = pd.DataFrame.from_records(uniq_sites_filtered)\n",
    "print(df['MM'].value_counts())\n",
    "print(df['MM'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9769\n",
      "5649\n"
     ]
    }
   ],
   "source": [
    "#save file\n",
    "df.to_csv(path_or_buf=path+\"new\"+\"all_uniq_by_frequncy_sites.known.tab\",index=False,sep='\\t')\n",
    "print(df.shape[0])\n",
    "# create and save AG sites only\n",
    "AG_df=pd.DataFrame.from_records(AG_uniq_sites_filtered)\n",
    "AG_df.to_csv(path_or_buf=path+\"new\"+\"all_uniq_by_frequncy_sites_A2G.known.tab\",index=False,sep='\\t')\n",
    "print(AG_df.shape[0])\n",
    "\n",
    "\n",
    "# df_A2G_1=df[df['MM']==\"AG\"]\n",
    "# print(df_A2G_1.shape[0])\n",
    "# df_A2G=df[df['AllSubs'].str.contains(\"AG\",regex=False)]\n",
    "# print(df_A2G.shape[0])\n",
    "# df.drop(['AllSubs'],axis=1,inplace=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3566c55396aa67d6d450caee2c72072e0a80bcedc801748038b289e6d3e79506"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lamootHW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
