{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/private8/Projects/itamar/ZNF/Sra_GSE73211_35_samples/REDI/curated/nodups_cov5_HE_PLUS_STAR100len/Edits_only/\"\n",
    "all_samples = []\n",
    "for outf in [path+f for f in os.listdir(path) if f.endswith('.out.tab')]:\n",
    "    with open(outf,'r') as ffd:\n",
    "        DictReader_obj = csv.DictReader(ffd, delimiter=\"\\t\")\n",
    "        # content = ffd.readlines()\n",
    "        file_as_list_dict = [line for line in DictReader_obj]\n",
    "        all_samples.append(file_as_list_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65653\n",
      "65300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#filter 100% editing sites\n",
    "all_files_sites_no_snp_raw =[]\n",
    "for sample in all_samples:\n",
    "        # temp = [site for site in sample if 0.001 < float(site[\"Frequency\"]) < 0.98] \n",
    "        # TODO also filte 45 -55%?\n",
    "        temp  = [site for site in sample if ((0.001 < float(site[\"Frequency\"]) < 0.98) and  (0.55 < float(site[\"Frequency\"]) or float(site[\"Frequency\"]) < 0.45))]\n",
    "        all_files_sites_no_snp_raw.append(temp.copy())\n",
    "\n",
    "#fiter snps from dbsnp\n",
    "temp_F_path = path+\"temp_pos.bed\"\n",
    "sites_as_bed =  [site[\"Region\"]+\"\\t\"+str(int(site[\"Position\"])-1)+\"\\t\"+site[\"Position\"]+\"\\n\" for sample in all_files_sites_no_snp_raw for site in sample]\n",
    "uniq = set(sites_as_bed)\n",
    "#print(len(uniq))\n",
    "with open(temp_F_path, \"w\") as outFD:\n",
    "    outFD.writelines(uniq)\n",
    "# intersect with snps\n",
    "snps = BedTool(\"/private/common/Data/dbSNP-153/hg38-dbSNP153-common.bed\")\n",
    "sites = BedTool(temp_F_path)\n",
    "result_path = path+\"temp_result.bed\"\n",
    "# write and read results\n",
    "(sites-snps).saveas(result_path)\n",
    "with open(result_path, \"r\") as resFD:\n",
    "    as_bed_no_snp = resFD.readlines()\n",
    "#print(len(as_bed_no_snp))\n",
    "# remove temp file\n",
    "os.remove(result_path)\n",
    "os.remove(temp_F_path)\n",
    "# save the result as positions\n",
    "no_snp_positions = [(pos.split(sep=\"\\t\")[0], pos.split(sep=\"\\t\")[2][:-1])for pos in as_bed_no_snp]\n",
    "all_files_sites_no_snp = []\n",
    "# save for every sample the passed sites\n",
    "for sample in all_files_sites_no_snp_raw:\n",
    "    all_files_sites_no_snp.append([site for site in sample if (site[\"Region\"], site[\"Position\"]) in no_snp_positions])\n",
    "# save list of all reults (contains duplicats!)\n",
    "all_sites_list_no_snp = [site for sample in all_files_sites_no_snp for site in sample]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create table of all sites composing data for every uniq site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  137393 \n",
      " uniq:  65300\n"
     ]
    }
   ],
   "source": [
    "# create uniq list of all sites\n",
    "uniq_sites = []\n",
    "sites_dict = dict()\n",
    "already = set()\n",
    "for site in all_sites_list_no_snp:\n",
    "    # make set from subs\n",
    "    site[\"AllSubs\"] = set(site[\"AllSubs\"].split(\" \"))\n",
    "    \n",
    "    # # keep only A2G sites\n",
    "    # if \"AG\" not in site[\"AllSubs\"]:\n",
    "    #     continue\n",
    "\n",
    "\n",
    "    site[\"BaseCount[A,C,G,T]\"]  = np.array([int(x) for x in site[\"BaseCount[A,C,G,T]\"].replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")])\n",
    "    site['Coverage-q25'] = float(site['Coverage-q25'])\n",
    "    site['MeanQ'] = float(site['MeanQ'])\n",
    "\n",
    "    siteID = site[\"Region\"]+\"_\"+site[\"Position\"] \n",
    "    if siteID not in sites_dict:\n",
    "        sites_dict[siteID] = site\n",
    "    else:\n",
    "        sites_dict[siteID][\"AllSubs\"].union(site[\"AllSubs\"])\n",
    "        sites_dict[siteID][\"BaseCount[A,C,G,T]\"] += site[\"BaseCount[A,C,G,T]\"]\n",
    "        total_cov = sites_dict[siteID]['Coverage-q25'] + site['Coverage-q25']\n",
    "        sites_dict[siteID]['MeanQ'] = round(site['MeanQ']*(site['Coverage-q25']/total_cov) + sites_dict[siteID]['MeanQ']*(sites_dict[siteID]['Coverage-q25']/total_cov),1)\n",
    "        sites_dict[siteID]['Coverage-q25'] += site['Coverage-q25']\n",
    "    # if (site[\"Region\"], site[\"Position\"]) not in already:\n",
    "    #     already.add((site[\"Region\"], site[\"Position\"]))\n",
    "    #     uniq_sites.append(site)\n",
    "    \n",
    "    # make resulte readable \n",
    "for site in sites_dict.values():\n",
    "    # calc frequncy again\n",
    "    b_c=site[\"BaseCount[A,C,G,T]\"].copy()\n",
    "    n_options = ['A','C','G','T']\n",
    "    #find the original nucleotide without editing\n",
    "    orig_pos=np.argmax(b_c)\n",
    "    orig_nuc=n_options[orig_pos]\n",
    "    b_c[orig_pos]=0\n",
    "    # find the edited to\n",
    "    new_pos=np.argmax(b_c)\n",
    "    new_nuc=n_options[new_pos]\n",
    "    # save mismatch type and Frequency\n",
    "    site['MM']=orig_nuc+new_nuc\n",
    "    site['Frequency']=round(site[\"BaseCount[A,C,G,T]\"][new_pos]/np.sum(site[\"BaseCount[A,C,G,T]\"]),4)\n",
    "    # convert BaseCount to string\n",
    "    site[\"BaseCount[A,C,G,T]\"] = \";\".join([str(x) for x in site[\"BaseCount[A,C,G,T]\"].tolist()])\n",
    "    site[\"AllSubs\"] = \";\".join(site[\"AllSubs\"])\n",
    "# convert dict to list\n",
    "uniq_sites = [site for site in sites_dict.values()]\n",
    "print(\"total: \",len(all_sites_list_no_snp),\"\\n uniq: \", len(uniq_sites))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter by frequncy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  137393 \n",
      " uniq:  9769\n"
     ]
    }
   ],
   "source": [
    "# filter by cov > 100 and some MM's freq > 0.01\n",
    "uniq_sites_filtered = [site for site in uniq_sites if (site[\"Frequency\"] >= 0.01 and site['Coverage-q25'] > 100)]\n",
    "print(\"total: \",len(all_sites_list_no_snp),\"\\n uniq: \", len(uniq_sites_filtered))\n",
    "# Keep only AG \n",
    "uniq_sites_filtered_AG = [site for site in uniq_sites_filtered if site[\"MM\"] == 'AG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG    5551\n",
      "AT    2339\n",
      "AC    1879\n",
      "Name: MM, dtype: int64\n",
      "AG    0.568226\n",
      "AT    0.239431\n",
      "AC    0.192343\n",
      "Name: MM, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# count number of each MM\n",
    "df = pd.DataFrame(uniq_sites_filtered)\n",
    "print(df['MM'].value_counts())\n",
    "print(df['MM'].value_counts(normalize=True))\n",
    "df.drop(['AllSubs'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save file\n",
    "df.to_csv(path_or_buf=path+\"all_uniq_by_frequncy_sites.known.tab\",index=False,sep='\\t')\n",
    "# with open(path+\"all_uniq_by_frequncy_sites.known.tab\", \"w\", newline='') as outFd:\n",
    "#     writer = csv.DictWriter(outFd, delimiter=\"\\t\",fieldnames=list(uniq_sites_filtered_AG[0].keys()),lineterminator='\\n')\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(uniq_sites_filtered_AG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path+\"all_uniq_by_frequncy_sites.known.tab\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9769\n",
      "5551\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "df_A2G=df[df['MM']==\"AG\"]\n",
    "print(df_A2G.shape[0])\n",
    "df_A2G.to_csv(path_or_buf=path+\"all_uniq_by_frequncy_sites_A2G.known.tab\",index=False,sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3566c55396aa67d6d450caee2c72072e0a80bcedc801748038b289e6d3e79506"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lamootHW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
